# Build Tests Prompt

Based on the scenarios from Stage 1 and the implementation code, generate a comprehensive test suite that validates the agent's functionality.

## Test Suite Generation

Create a complete test framework that covers all scenarios and implementation components:

### Main Test Suite (test-suite.py)

```python
#!/usr/bin/env python3
"""
Comprehensive Test Suite for Generated Agent

This test suite validates all agent functionality against the defined scenarios
and ensures the implementation meets SOP requirements.
Generated by Claude SOP Generator.
"""

import pytest
import asyncio
import json
import time
from typing import Dict, Any, List
from unittest.mock import Mock, AsyncMock, patch
from dataclasses import dataclass

# Import the agent implementation
try:
    from agent_core import Agent, AgentConfig, ValidationError, ProcessingError
    from integrations import IntegrationManager, IntegrationError
except ImportError:
    # Handle case where modules aren't available yet
    pass

@pytest.fixture
def agent_config():
    """Test configuration for agent"""
    return AgentConfig(
        name="Test Agent",
        version="1.0.0-test",
        timeout=5,
        max_retries=1,
        cache_ttl=10,
        debug_mode=True
    )

@pytest.fixture
async def agent(agent_config):
    """Create agent instance for testing"""
    agent_instance = Agent(agent_config)
    await agent_instance.integrations.initialize()
    yield agent_instance
    await agent_instance.integrations.shutdown()

@pytest.fixture
def mock_integrations():
    """Mock external integrations"""
    with patch('integrations.IntegrationManager') as mock:
        manager = Mock()
        manager.initialize = AsyncMock()
        manager.shutdown = AsyncMock()
        manager.get_integration = AsyncMock()
        mock.return_value = manager
        yield manager

class TestAgentCore:
    """Test the main agent functionality"""
    
    @pytest.mark.asyncio
    async def test_simple_request_processing(self, agent):
        """Test Scenario 1: Simple request processing"""
        # Arrange
        request = {
            "type": "simple",
            "data": {"message": "Hello, World!"}
        }
        
        # Act
        response = await agent.process_request(request)
        
        # Assert
        assert response["status"] == "success"
        assert "request_id" in response
        assert "data" in response
        assert response["data"]["result"] == "Simple processing completed"
    
    @pytest.mark.asyncio
    async def test_complex_request_processing(self, agent):
        """Test Scenario 2: Complex request processing"""
        # Arrange
        request = {
            "type": "complex",
            "data": {
                "parameters": {"complexity": "high"},
                "requirements": ["analysis", "integration"]
            }
        }
        
        # Act
        response = await agent.process_request(request)
        
        # Assert
        assert response["status"] == "success"
        assert "Complex processing completed" in response["data"]["result"]
    
    @pytest.mark.asyncio
    async def test_invalid_request_handling(self, agent):
        """Test Scenario 3: Invalid input handling"""
        # Arrange
        invalid_requests = [
            {},  # Empty request
            {"data": "no type"},  # Missing type
            {"type": ""},  # Empty type
            "not a dict",  # Wrong format
        ]
        
        for invalid_request in invalid_requests:
            # Act
            response = await agent.process_request(invalid_request)
            
            # Assert
            assert response["status"] == "error"
            assert "error" in response
    
    @pytest.mark.asyncio
    async def test_request_timeout_handling(self, agent):
        """Test Scenario 4: Timeout handling"""
        # Arrange
        with patch.object(agent.processor, 'process', side_effect=asyncio.TimeoutError()):
            request = {"type": "simple", "data": {"message": "test"}}
            
            # Act
            response = await agent.process_request(request)
            
            # Assert
            assert response["status"] == "error"
            assert "timeout" in response["error"]["message"].lower()
    
    @pytest.mark.asyncio
    async def test_concurrent_request_handling(self, agent):
        """Test Scenario 5: Concurrent processing"""
        # Arrange
        requests = [
            {"type": "simple", "data": {"id": i}}
            for i in range(10)
        ]
        
        # Act
        tasks = [agent.process_request(req) for req in requests]
        responses = await asyncio.gather(*tasks)
        
        # Assert
        assert len(responses) == 10
        for response in responses:
            assert response["status"] == "success"
        
        # Verify unique request IDs
        request_ids = [resp["request_id"] for resp in responses]
        assert len(set(request_ids)) == 10

class TestInputValidation:
    """Test input validation functionality"""
    
    @pytest.mark.asyncio
    async def test_schema_validation(self, agent):
        """Test input schema validation"""
        test_cases = [
            # Valid cases
            ({"type": "simple", "data": {}}, True),
            ({"type": "complex", "data": {"key": "value"}}, True),
            
            # Invalid cases
            ({}, False),  # Missing required fields
            ({"type": "simple"}, False),  # Missing data
            ({"data": {}}, False),  # Missing type
        ]
        
        for request, should_succeed in test_cases:
            response = await agent.process_request(request)
            if should_succeed:
                assert response["status"] == "success"
            else:
                assert response["status"] == "error"
    
    @pytest.mark.asyncio
    async def test_business_rule_validation(self, agent):
        """Test business rule validation"""
        # Test specific business rules from SOP
        request = {
            "type": "business_rule_test",
            "data": {"value": "test_value"}
        }
        
        response = await agent.process_request(request)
        
        # Assert business rules are applied
        assert response["status"] in ["success", "error"]

class TestWorkflowProcessing:
    """Test SOP workflow implementation"""
    
    @pytest.mark.asyncio
    async def test_decision_tree_routing(self, agent):
        """Test decision tree routing logic"""
        test_cases = [
            {"type": "simple", "expected_handler": "simple"},
            {"type": "complex", "expected_handler": "complex"},
            {"type": "unknown", "expected_handler": "default"},
        ]
        
        for case in test_cases:
            request = {"type": case["type"], "data": {}}
            response = await agent.process_request(request)
            
            assert response["status"] == "success"
            # Verify correct handler was used (check in response or logs)
    
    @pytest.mark.asyncio
    async def test_state_management(self, agent):
        """Test workflow state management"""
        # Test that agent maintains proper state through processing
        request = {"type": "stateful", "data": {"session_id": "test123"}}
        
        response = await agent.process_request(request)
        
        assert response["status"] == "success"
        # Verify state was properly managed
    
    @pytest.mark.asyncio
    async def test_error_recovery(self, agent):
        """Test error recovery procedures"""
        # Simulate various error conditions
        with patch.object(agent.processor, 'process', side_effect=Exception("Test error")):
            request = {"type": "simple", "data": {}}
            
            response = await agent.process_request(request)
            
            assert response["status"] == "error"
            assert "Test error" in str(response["error"])

class TestIntegrations:
    """Test external integrations"""
    
    @pytest.mark.asyncio
    async def test_integration_success(self, agent, mock_integrations):
        """Test successful integration calls"""
        # Mock successful integration response
        mock_api = AsyncMock()
        mock_api.get_item.return_value = {"id": "123", "data": "test"}
        mock_integrations.get_integration.return_value = mock_api
        
        # Test integration call through agent
        request = {
            "type": "integration_test",
            "data": {"item_id": "123"}
        }
        
        response = await agent.process_request(request)
        
        assert response["status"] == "success"
    
    @pytest.mark.asyncio
    async def test_integration_failure_handling(self, agent, mock_integrations):
        """Test integration failure handling"""
        # Mock integration failure
        mock_api = AsyncMock()
        mock_api.get_item.side_effect = IntegrationError("Service unavailable")
        mock_integrations.get_integration.return_value = mock_api
        
        request = {
            "type": "integration_test",
            "data": {"item_id": "123"}
        }
        
        response = await agent.process_request(request)
        
        # Should handle integration error gracefully
        assert response["status"] in ["error", "success"]
    
    @pytest.mark.asyncio
    async def test_integration_retry_logic(self, agent, mock_integrations):
        """Test integration retry mechanism"""
        # Mock integration that fails then succeeds
        mock_api = AsyncMock()
        mock_api.get_item.side_effect = [
            IntegrationError("Temporary failure"),
            {"id": "123", "data": "test"}
        ]
        mock_integrations.get_integration.return_value = mock_api
        
        request = {
            "type": "integration_test",
            "data": {"item_id": "123"}
        }
        
        response = await agent.process_request(request)
        
        assert response["status"] == "success"
        assert mock_api.get_item.call_count == 2

class TestPerformance:
    """Test performance requirements"""
    
    @pytest.mark.asyncio
    async def test_response_time(self, agent):
        """Test response time requirements"""
        request = {"type": "simple", "data": {"message": "performance test"}}
        
        start_time = time.time()
        response = await agent.process_request(request)
        end_time = time.time()
        
        processing_time = end_time - start_time
        
        assert response["status"] == "success"
        assert processing_time < 5.0  # Should respond within 5 seconds
    
    @pytest.mark.asyncio
    async def test_memory_usage(self, agent):
        """Test memory usage during processing"""
        import psutil
        import os
        
        process = psutil.Process(os.getpid())
        initial_memory = process.memory_info().rss
        
        # Process multiple requests
        requests = [
            {"type": "simple", "data": {"id": i}}
            for i in range(100)
        ]
        
        for request in requests:
            await agent.process_request(request)
        
        final_memory = process.memory_info().rss
        memory_increase = final_memory - initial_memory
        
        # Memory increase should be reasonable (less than 50MB)
        assert memory_increase < 50 * 1024 * 1024
    
    @pytest.mark.asyncio
    async def test_throughput(self, agent):
        """Test request throughput"""
        num_requests = 50
        requests = [
            {"type": "simple", "data": {"id": i}}
            for i in range(num_requests)
        ]
        
        start_time = time.time()
        tasks = [agent.process_request(req) for req in requests]
        responses = await asyncio.gather(*tasks)
        end_time = time.time()
        
        processing_time = end_time - start_time
        throughput = num_requests / processing_time
        
        # Should handle at least 10 requests per second
        assert throughput >= 10
        
        # All requests should succeed
        assert all(resp["status"] == "success" for resp in responses)

class TestScenarios:
    """Test specific scenarios from Stage 1"""
    
    @pytest.mark.asyncio
    async def test_scenario_001(self, agent):
        """Test Scenario 001: [Specific scenario from SOP]"""
        # Implement test based on specific scenario
        request = {
            "type": "scenario_001",
            "data": {"test_parameter": "value"}
        }
        
        response = await agent.process_request(request)
        
        # Verify scenario-specific success criteria
        assert response["status"] == "success"
        # Add scenario-specific assertions
    
    @pytest.mark.asyncio
    async def test_edge_cases(self, agent):
        """Test edge cases identified in scenarios"""
        edge_cases = [
            # Empty data
            {"type": "simple", "data": {}},
            # Large data
            {"type": "simple", "data": {"large_field": "x" * 10000}},
            # Special characters
            {"type": "simple", "data": {"special": "!@#$%^&*()"}},
        ]
        
        for case in edge_cases:
            response = await agent.process_request(case)
            assert response["status"] in ["success", "error"]
            # Should not crash or cause unexpected behavior

class TestSecurityAndCompliance:
    """Test security and compliance requirements"""
    
    @pytest.mark.asyncio
    async def test_input_sanitization(self, agent):
        """Test input sanitization"""
        malicious_inputs = [
            {"type": "simple", "data": {"script": "<script>alert('xss')</script>"}},
            {"type": "simple", "data": {"sql": "'; DROP TABLE users; --"}},
            {"type": "simple", "data": {"command": "$(rm -rf /)"}},
        ]
        
        for malicious_input in malicious_inputs:
            response = await agent.process_request(malicious_input)
            
            # Should handle malicious input safely
            assert response["status"] in ["success", "error"]
            # Verify no code execution or injection occurred
    
    @pytest.mark.asyncio
    async def test_data_privacy(self, agent):
        """Test data privacy handling"""
        sensitive_request = {
            "type": "privacy_test",
            "data": {
                "personal_info": "sensitive_data",
                "ssn": "123-45-6789"
            }
        }
        
        response = await agent.process_request(sensitive_request)
        
        # Verify sensitive data is handled appropriately
        response_str = json.dumps(response)
        assert "123-45-6789" not in response_str  # Should not leak sensitive data

# Test runners and utilities
def run_performance_tests():
    """Run performance-specific tests"""
    pytest.main(["-v", "-k", "TestPerformance"])

def run_scenario_tests():
    """Run scenario-specific tests"""
    pytest.main(["-v", "-k", "TestScenarios"])

def run_integration_tests():
    """Run integration tests"""
    pytest.main(["-v", "-k", "TestIntegrations"])

def run_all_tests():
    """Run complete test suite"""
    pytest.main(["-v", "--tb=short"])

if __name__ == "__main__":
    # Run tests based on command line argument
    import sys
    
    if len(sys.argv) > 1:
        test_type = sys.argv[1]
        if test_type == "performance":
            run_performance_tests()
        elif test_type == "scenarios":
            run_scenario_tests()
        elif test_type == "integration":
            run_integration_tests()
        else:
            run_all_tests()
    else:
        run_all_tests()
```

## Test Configuration Files

### pytest.ini
```ini
[tool:pytest]
asyncio_mode = auto
testpaths = .
python_files = test_*.py *_test.py test-*.py
python_classes = Test*
python_functions = test_*
addopts = -v --tb=short --strict-markers
markers =
    asyncio: marks tests as async
    slow: marks tests as slow running
    integration: marks tests as integration tests
    performance: marks tests as performance tests
```

### conftest.py
```python
import pytest
import asyncio

@pytest.fixture(scope="session")
def event_loop():
    """Create an instance of the default event loop for the test session."""
    loop = asyncio.get_event_loop_policy().new_event_loop()
    yield loop
    loop.close()
```

## Test Data Files

### test_data.json
```json
{
  "valid_requests": [
    {"type": "simple", "data": {"message": "test"}},
    {"type": "complex", "data": {"parameters": {"key": "value"}}}
  ],
  "invalid_requests": [
    {},
    {"type": ""},
    {"data": "no_type"}
  ],
  "performance_data": {
    "small_request": {"type": "simple", "data": {"size": "small"}},
    "large_request": {"type": "simple", "data": {"content": "x".repeat(1000)}}
  }
}
```

## Coverage and Quality Checks

### Coverage Configuration (.coveragerc)
```ini
[run]
source = .
omit = 
    test_*.py
    *_test.py
    */tests/*
    */venv/*

[report]
exclude_lines =
    pragma: no cover
    def __repr__
    if self.debug:
    if settings.DEBUG
    raise AssertionError
    raise NotImplementedError
```

Generate comprehensive tests that validate all agent functionality against the defined scenarios and ensure production readiness.